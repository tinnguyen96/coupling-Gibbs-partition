{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '../modules/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "from time import time\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../modules/\")\n",
    "import utils\n",
    "import unbiased_estimation\n",
    "\n",
    "import imp\n",
    "imp.reload(unbiased_estimation)\n",
    "imp.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_clusts(data, clusts, ax=None):\n",
    "    if ax == None:\n",
    "        ax = plt.subplot(111)\n",
    "        show=True\n",
    "    else:\n",
    "        show = False\n",
    "        \n",
    "    for idcs in clusts:\n",
    "        idcs = list(idcs)\n",
    "        x_clust = data[idcs]\n",
    "        ax.scatter(x_clust[:, 0], x_clust[:, 1])\n",
    "    lim = np.max(np.abs(data))\n",
    "    ax.set_xlim([-lim,lim])\n",
    "    ax.set_ylim([-lim,lim])\n",
    "    \n",
    "    if show: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data structure for state\n",
    "# z : N-vector of assignments\n",
    "# clusts : list of lists of cluster assignments\n",
    "# for each cluster c, z[clusts[c]] is a vector of c's\n",
    "\n",
    "def pop_idx(z, clusts, n):\n",
    "    \"\"\"pop_idx removes the datapoint n from the current cluster assignments\n",
    "    \n",
    "    Args:\n",
    "        z: list of cluster assignments\n",
    "        clusts: list clusters (each cluster is a set)\n",
    "        n: index of datapoint to remove\n",
    "    \n",
    "    Returns:\n",
    "        updated labels, clusters, and index of removed cluster (or None if no cluster was removed)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    c = z[n]\n",
    "    z[n] = -1\n",
    "    clusts[c].remove(n)\n",
    "    \n",
    "    # if cluster is empty, remove it and reassign others\n",
    "    if len(clusts[c]) == 0:\n",
    "        removed_cluster = c\n",
    "        for i in range(c, len(clusts)-1):\n",
    "            clusts[i] = clusts[i+1]\n",
    "            z[list(clusts[i])] = i\n",
    "        clusts.pop() # drop last cluster\n",
    "    else:\n",
    "        removed_cluster = None\n",
    "    return z, clusts, removed_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_sweep_single(data, z, sd, sd0, alpha=0.01):\n",
    "    clusts = utils.z_to_clusts(z)  # initial data counts at each cluster\n",
    "    # take a Gibbs step at each data point\n",
    "    for n in range(len(data)):\n",
    "        # get rid of the nth data point and relabel clusters if needed\n",
    "        z, clusts, _ = pop_idx(z, clusts, n)\n",
    "\n",
    "        # sample which cluster this point should belong to\n",
    "        loc_probs = utils.weights_n(data, clusts, sd, sd0, alpha, n)\n",
    "        newz = np.random.choice(len(loc_probs), p=loc_probs)\n",
    "\n",
    "        # if necessary, instantiate a new cluster\n",
    "        if newz == len(clusts): clusts.append(set())\n",
    "\n",
    "        # update cluster assigments\n",
    "        clusts[newz].add(n)\n",
    "        z[n] = newz\n",
    "\n",
    "    return z\n",
    "\n",
    "def crp_gibbs(data, sd, sd0, initz, alpha=0.01, plot=True, log_freq=None, maxIters=100):\n",
    "    \"\"\"crp_gibbs runs a gibbs sampler \n",
    "    \n",
    "    The state of the chain includes z and clusts, which are redundant \n",
    "    but make for easy / fast indexing.\n",
    "    \n",
    "    just setting alpha for inference.\n",
    "    a small alpha encourages a small number of clusters.\n",
    "    \n",
    "    Returns:\n",
    "        z  (array of cluster assignments) and clusts (list of clusters)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the sampler\n",
    "    z = np.array(initz) # don't overwrite\n",
    "    clusts = utils.z_to_clusts(z)  # initial data counts at each cluster\n",
    "    \n",
    "    # set frequency at which to log state of the chain\n",
    "    if log_freq is None: log_freq = int(maxIters/10)\n",
    "    \n",
    "    # run the Gibbs sampler\n",
    "    for I in range(maxIters):\n",
    "        # take a Gibbs step at each data point\n",
    "        z = gibbs_sweep_single(data, z.copy(), sd, sd0, alpha=alpha)\n",
    "        \n",
    "        if (I%log_freq==0 or I==maxIters-1) and plot:\n",
    "            clusts = utils.z_to_clusts(z)  # initial data counts at each cluster\n",
    "            print(\"Iteration %04d/%04d\"%(I, maxIters))\n",
    "            plot_clusts(data, clusts, ax=None)\n",
    "    return z, utils.z_to_clusts(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_sweep_couple(data, z1, z2, sd, sd0, alpha=0.01, coupling=\"Maximal\"):\n",
    "    \"\"\"gibbs_sweep_couple performs Gibbs updates for every label in sequence, \n",
    "    coupling each update across the two chains.\n",
    "    \n",
    "    We compute intersection sizes once at the start and then update it for better time complexity.\n",
    "    \"\"\"\n",
    "    # Compute clusters and intersection sizes from scratch once\n",
    "    clusts1, clusts2 = utils.z_to_clusts(z1), utils.z_to_clusts(z2)  # initial data counts at each cluster\n",
    "    intersection_sizes = np.array([[len(c1.intersection(c2)) for c2 in clusts2] for c1 in clusts1])\n",
    "    \n",
    "    # Take a Gibbs step at each data point\n",
    "    for n in range(len(data)):\n",
    "        # Get rid of the nth data point and relabel clusters if needed\n",
    "        z1, clusts1, removed_clust1 = pop_idx(z1, clusts1, n)\n",
    "        z2, clusts2, removed_clust2 = pop_idx(z2, clusts2, n)\n",
    "        \n",
    "        # Update intersection sizes with datapoint n removed\n",
    "        if (removed_clust1 is None) and (removed_clust2 is None):\n",
    "            # If neither cluster was removed, decrement size of intersection\n",
    "            intersection_sizes[z1[n],z2[n]] -= 1\n",
    "        else:\n",
    "            # Otherwise remove corresponding row and or column\n",
    "            if removed_clust1 is not None:\n",
    "                intersection_sizes = np.delete(intersection_sizes, removed_clust1, axis=0)\n",
    "            if removed_clust2 is not None:\n",
    "                intersection_sizes = np.delete(intersection_sizes, removed_clust2, axis=1)\n",
    "            \n",
    "        # Compute marginal probabilities\n",
    "        loc_probs1 = utils.weights_n(data, clusts1, sd, sd0, alpha, n)\n",
    "        loc_probs2 = utils.weights_n(data, clusts2, sd, sd0, alpha, n)\n",
    "\n",
    "        # Sample new clusters assignments from coupling\n",
    "        if coupling==\"Common_RNG\":\n",
    "            newz1, newz2 = utils.naive_coupling(loc_probs1, loc_probs2)\n",
    "        elif coupling==\"Maximal\":\n",
    "            newz1, newz2 = utils.max_coupling(loc_probs1, loc_probs2)\n",
    "        else:\n",
    "            assert coupling==\"Optimal\"\n",
    "            pairwise_dists = utils.pairwise_dists(clusts1, clusts2, intersection_sizes)\n",
    "            _, (newz1, newz2), _ = utils.optimal_coupling(\n",
    "                loc_probs1, loc_probs2, pairwise_dists, normalize=True,\n",
    "                change_size=100)\n",
    "\n",
    "        # if necessary, instantiate a new cluster and pad intersection_sizes appropriately\n",
    "        if newz1 == len(clusts1):\n",
    "            clusts1.append(set())\n",
    "            intersection_sizes = utils.pad_with_zeros(intersection_sizes, axis=0)\n",
    "        if newz2 == len(clusts2):\n",
    "            clusts2.append(set())\n",
    "            intersection_sizes = utils.pad_with_zeros(intersection_sizes, axis=1)\n",
    "\n",
    "        # update cluster assigments and intersection sizes\n",
    "        clusts1[newz1].add(n); clusts2[newz2].add(n)\n",
    "        z1[n], z2[n] = newz1, newz2\n",
    "        intersection_sizes[z1[n],z2[n]] += 1\n",
    "        \n",
    "    return z1, z2\n",
    "\n",
    "def crp_gibbs_couple(\n",
    "    data, sd, sd0, initz1, initz2,alpha=0.01, plot=True,\n",
    "    log_freq=None, maxIters=100, coupling=\"Maximal\", save_base=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        coupling: method of coupling must be \"Common_RNG\", \"Maximal\" or \"Optimal\" (\"Common_RNG\" used to be \"Naive\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the sampler\n",
    "    z1, z2 = initz1, initz2\n",
    "    z1s, z2s = [z1.copy()], [z2.copy()]\n",
    "    \n",
    "    dists_by_iter = []\n",
    "    \n",
    "    # set frequency at which to log state of the chain\n",
    "    if log_freq is None: log_freq = int(maxIters/10)\n",
    "    \n",
    "    # run the Gibbs sampler\n",
    "    for I in range(maxIters):\n",
    "            \n",
    "        z1, z2 = gibbs_sweep_couple(\n",
    "            data, z1.copy(), z2.copy(), sd, sd0,\n",
    "            alpha=alpha)\n",
    "            \n",
    "        # data counts at each cluster\n",
    "        clusts1, clusts2 = utils.z_to_clusts(z1), utils.z_to_clusts(z2)  \n",
    "        z1s.append(z1); z2s.append(z2)\n",
    "        \n",
    "        \n",
    "        dist_between_partitions = utils.adj_dists_fast(clusts1, clusts2)\n",
    "        dists_by_iter.append(dist_between_partitions)\n",
    "        \n",
    "        if (I%log_freq==0 or dist_between_partitions==0) and plot:\n",
    "            print(\"Iteration %04d/%04d\"%(I, maxIters))\n",
    "            print(\"n_clusts: \", len(clusts1), len(clusts2))\n",
    "            save_name = save_base + \"_%04d.png\"%I if save_base is not None else None\n",
    "            plot_chains(data, clusts1, clusts2, save_name=save_name)\n",
    "            \n",
    "        if dist_between_partitions == 0:\n",
    "            print(\"Chains coupled after %d iterations!\"%I)\n",
    "            break\n",
    "        \n",
    "    return z1, dists_by_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and set hyper-params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ex6_gen_data(Ndata, sd, sd0=1, K=2):\n",
    "    # TRANSLATION OF TAMARA's CODE INTO PYTHON\n",
    "    #\n",
    "    # generate Gaussian mixture model data for inference later\n",
    "    #\n",
    "    # Args:\n",
    "    #  Ndata: number of data points to generate\n",
    "    #  sd: covariance matrix of data points around the\n",
    "    #      cluster-specific mean is [sd^2, 0; 0, sd^2];\n",
    "    #      i.e. this is the standard deviation in either direction\n",
    "    #  sd0: std for prior mean\n",
    "    #\n",
    "    # Returns:\n",
    "    #  x: an Ndata x 2 matrix of data points\n",
    "    #  z: an Ndata-long vector of cluster assignments\n",
    "    #  mu: a K x 2 matrix of cluster means,\n",
    "    #      where K is the number of clusters\n",
    "\n",
    "    # matrix of cluster centers: one in each quadrant\n",
    "    mu = np.random.normal(scale=sd0, size=[K, 2])\n",
    "    # vector of component frequencies\n",
    "    #rho = np.array([0.4,0.3,0.2,0.1])\n",
    "    rho = stats.dirichlet.rvs(alpha=2*np.ones(K))[0]\n",
    "\n",
    "    # assign each data point to a component\n",
    "    z = np.random.choice(range(K), p=rho, replace=True, size=Ndata)\n",
    "    # draw each data point according to the cluster-specific\n",
    "    # likelihood of its component\n",
    "    x = mu[z] + np.random.normal(scale=sd, size=[Ndata,2])  \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "np.random.seed(44)\n",
    "K = 3\n",
    "Ndata = 50\n",
    "sd, sd0, alpha = 2., 2., 0.5\n",
    "data = ex6_gen_data(Ndata, sd, sd0, K=K)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene-expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ndata = 200\n",
    "D = 50\n",
    "\n",
    "sd, sd0, alpha = 2., 2., 0.5 # totally arbitrary\n",
    "data = np.loadtxt('data/gene_data_Ndata=%d_D=%d.csv' %(Ndata,D),delimiter=',',skiprows=1,usecols=range(1,D+1))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions acting on partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unbiased estimation\n",
    "def posterior_predictive_density(data, z, data_new, sd, sd0, alpha):\n",
    "    \"\"\"posterior_predictive_density computes the posterior predictive density, conditioned \n",
    "    on z and data at data_new (for the DP-mixture model).\n",
    "    \n",
    "    Args:\n",
    "        data: observations\n",
    "        z: assignments\n",
    "        data_new: location of single observation at which to compute posterior predictive\n",
    "        sd, sd0: standard deviation of noise and prior on means\n",
    "        alpha: DP paramter\n",
    "        \n",
    "    Returns:\n",
    "        posterior predictive density at data_new\n",
    "    \"\"\"\n",
    "    Ndata = len(z)\n",
    "    Ndata_new, D = data_new.shape\n",
    "    \n",
    "    # Initialize Posterior Predictive Density (ppd) as zeros \n",
    "    ppd = np.zeros(Ndata_new)\n",
    "    \n",
    "    # Add density corresponding to new cluster\n",
    "    prob_new_cluster = alpha / (Ndata+alpha)\n",
    "    ppd += prob_new_cluster * stats.multivariate_normal(\n",
    "        mean=np.zeros(D), cov=(sd**2 + sd0**2)*np.eye(D)).pdf(data_new)\n",
    "    \n",
    "    for clust in set(np.unique(z)):\n",
    "        # pull out data in cluster 'clust'\n",
    "        data_c = data[np.where(z==clust)[0]]\n",
    "        \n",
    "        # probability that new datapoint is in clust\n",
    "        prob_clust = len(data_c)/(Ndata + alpha)\n",
    "        \n",
    "        # posterior over mean of clust\n",
    "        post_prec = (1./(sd0**2)) + len(data_c)*(1./(sd**2))\n",
    "        post_var = 1./post_prec\n",
    "        post_mean = post_var*(len(data_c)*(1./(sd**2))*np.mean(data_c,axis=0))\n",
    "        \n",
    "        ppd += prob_clust * stats.multivariate_normal(\n",
    "            mean=post_mean, cov=(post_var + sd**2)*np.eye(D)).pdf(data_new)\n",
    "    return ppd\n",
    "\n",
    "def posterior_predictive_density_grid(data, z, sd, sd0, alpha, n_grid_spaces=20):\n",
    "    \"\"\"posterior_predictive_density_grid evaluate the posterior predictive density at a grid of points.\n",
    "    \n",
    "    Args:\n",
    "        (Same as posterior_predictive_density)\n",
    "        n_grid_spaces: granularity of locations at which to compute density\n",
    "    \"\"\"\n",
    "    scale = 2*sd**2 + 2*sd0**2\n",
    "    delta = scale/10\n",
    "    \n",
    "    # Create vector of locations at which to query predictive distribution\n",
    "    x, y = np.arange(-scale, scale, delta), np.arange(-scale, scale, delta)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    X_long, Y_long = X.reshape([-1]), Y.reshape([-1])\n",
    "    data_new = np.array(list(zip(X_long, Y_long)))\n",
    "    \n",
    "    # Compute predictive density and reshape into a grid for easy visualization\n",
    "    ppd = posterior_predictive_density(data, z, data_new, sd, sd0, alpha)\n",
    "    ppd_grid = ppd.reshape(X.shape)\n",
    "    X_Y_ppd = np.array([X, Y, ppd_grid])\n",
    "    return X_Y_ppd\n",
    "\n",
    "def plot_density(X, Y, Z, title=None):\n",
    "    \"\"\"plot_density shows a predictive density with a contour plot.\n",
    "    \n",
    "    Args:\n",
    "        X, Y: x and y positions of density values, both np.array of shape [n_grid_spaces, n_grid_spaces]\n",
    "        Z: density values\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    CS = ax.contour(X, Y, Z)\n",
    "    ax.clabel(CS, inline=1, fontsize=10)\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top K proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prop_in_k_clusters(z, k=10):\n",
    "    # compute cluster sizes in decreasing order.\n",
    "    clusts = utils.z_to_clusts(z)\n",
    "    clust_sizes = list(sorted([len(clust) for clust in clusts], reverse=True))\n",
    "    \n",
    "    # compute proportion of datapoints assigned to up to top k clusters.\n",
    "    props = np.ones(shape=[k])\n",
    "    props[:len(clust_sizes)] = np.cumsum(clust_sizes)[:k]/len(z)\n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = np.array([0,0,0,0,1,1,1,2,2])\n",
    "clusts = utils.z_to_clusts(z)\n",
    "clusts_as_lists = [list(clust) for clust in clusts]\n",
    "clusts_sizes_with_labels = [(len(clust),z[clust[0]]) for clust in clusts_as_lists]\n",
    "\n",
    "print(clusts_as_lists)\n",
    "print(len(clusts_as_lists[0]))\n",
    "print(clusts_sizes_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-clustering matrix\n",
    "- a thin wrapper around utils.adj_matrix since adjacency matrix AKA co-clustering matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_cluster(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        z: (N,) vector of labels\n",
    "    Output:\n",
    "        mat: (N,N) matrix of whether two observations are in the same cluster\n",
    "    \"\"\"\n",
    "    clusts = utils.z_to_clusts(z)\n",
    "    mat = utils.adj_matrix(z, clusts)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define utilities for performing unbiased estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crp_prior(N,alpha):\n",
    "    \"\"\"\n",
    "    There's got to be a simpler way to do this.\n",
    "    \n",
    "    Input:\n",
    "        N: number of observations\n",
    "        alpha: scalar, \n",
    "    \n",
    "    Output:\n",
    "        a sample from CRP prior\n",
    "    \"\"\"\n",
    "    z0 = np.array([0])\n",
    "    for n in range(1,N):\n",
    "        # print(n)\n",
    "        clusts = utils.z_to_clusts(z0)\n",
    "        # print(clusts)\n",
    "        clusts_as_lists = [list(clust) for clust in clusts]\n",
    "        # print(clusts_as_lists)\n",
    "        clusts_sizes_with_labels = [(len(clust),z0[clust[0]]) for clust in clusts_as_lists]\n",
    "        probs_with_label = [(clust[0]/(n+alpha), clust[1]) for clust in clusts_sizes_with_labels] # adding to old clusters\n",
    "        probs_with_label.append((alpha/(n+alpha),len(clusts))) # new cluster \n",
    "        probs = [x[0] for x in probs_with_label]\n",
    "        # print(probs)\n",
    "        new_point = probs_with_label[np.random.choice(len(probs), p=probs)][1]\n",
    "        z0 = np.append(z0,new_point)\n",
    "        # print(z0)\n",
    "    return z0\n",
    "\n",
    "def pi0(data, sd, sd0, alpha, pi0_its=0):\n",
    "    \"\"\"pi0 is the initial distribution for the Markov chain.\n",
    "    \"\"\"\n",
    "    Ndata = data.shape[0]\n",
    "    z0 = np.zeros(Ndata, dtype=np.int)\n",
    "    for i in range(pi0_its): z0 = gibbs_sweep_single(data, z0.copy(), sd, sd0, alpha)\n",
    "    return z0\n",
    "\n",
    "def unbiased_est_crp(k, h, m, data, sd, sd0, alpha, pi0_its=0, init_type=\"crp_prior\", coupling=\"Optimal\"):#\n",
    "    \"\"\"unbiased_est produces an unbiased estimate of a functional using the approach of Jacob 2020\n",
    "    \n",
    "    initializes from r_init\n",
    "    \n",
    "    Args:\n",
    "        k: # burn-in iterations\n",
    "        h: lambda function of interest (of labelings z)\n",
    "        m: minimum iterations\n",
    "        data: observations\n",
    "        pi0_its: number of iterations to run before coupling\n",
    "        coupling: either maximal or optimal\n",
    "    \n",
    "    Returns: \n",
    "        unbiased estimate, number of steps before meeting, states of either chain until meeting\n",
    "    \"\"\"\n",
    "    # Define initial distribution\n",
    "    if init_type == \"pi0\":\n",
    "        pi0_dp = lambda : pi0(data, sd, sd0, alpha, pi0_its)\n",
    "    elif init_type == \"crp_prior\":\n",
    "        pi0_dp = lambda : crp_prior(data.shape[0], alpha)\n",
    "    \n",
    "    # Define marginal and coupled transitions\n",
    "    gibbs_sweep = lambda z: gibbs_sweep_single(data, z.copy(), sd, sd0, alpha)\n",
    "    coupled_gibbs_sweep = lambda z1, z2: gibbs_sweep_couple(\n",
    "        data, z1.copy(), z2.copy(), sd, sd0, alpha, coupling=coupling)\n",
    "    \n",
    "    # Run coupled chains\n",
    "    X, Y, tau = unbiased_estimation.run_two_chains(m, pi0_dp, gibbs_sweep, coupled_gibbs_sweep)\n",
    "    \n",
    "    # Compute unbiased estimate\n",
    "    H_km = unbiased_estimation.unbiased_est(k, h, m, X, Y, tau)\n",
    "\n",
    "    return H_km, tau, X, Y\n",
    "\n",
    "# Usual MCMC estimate for comparison. Should we do thinning of the single ergodic average?\n",
    "def usual_MCMC_est_crp(k, h, m, data, sd, sd0, alpha, init_type=\"crp_prior\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        k: # burn-in iterations\n",
    "        h: lambda function of interest (of labelings z)\n",
    "        m: scalar, number of sweeps\n",
    "        data: observations\n",
    "        sd, sd0, alpha: hyper-params\n",
    "    \"\"\"\n",
    "    if (init_type == \"all_same\"):\n",
    "        X = [np.zeros(data.shape[0], dtype=int)]\n",
    "    elif (init_type == \"crp_prior\"):\n",
    "        X = [crp_prior(data.shape[0],alpha)]\n",
    "    for _ in range(m): X.append(gibbs_sweep_single(data, X[-1].copy(), sd, sd0, alpha))\n",
    "    ests = [h(x) for x in X[k:]]\n",
    "    mean = np.mean(ests, axis=0)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "N = 200\n",
    "alpha = 0.5\n",
    "\n",
    "z = crp_prior(N,alpha)\n",
    "print(utils.z_to_clusts(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_rep(est_type, h, maxIter, time_budget):\n",
    "    \"\"\"\n",
    "    Only keep track of time to traverse the posterior, not time\n",
    "    to actually evaluate the function.\n",
    "    \n",
    "    Input:\n",
    "        est_type: \"truth\", \"single\", \"coupled\"\n",
    "        h: lambda function\n",
    "        maxIter: scalar \n",
    "        time_budget: scalar, \n",
    "        \n",
    "    Return \n",
    "        states: list, only meaning full for the single or coupled estimators \n",
    "            (since we don't run for too long, not too memory intensive to save the Markov chains' \n",
    "            states)\n",
    "        result: scalar, estimate of the integral of interest\n",
    "        num_iter: number of estimators generated in the time_budget \n",
    "            (only interesting for est_type = \"coupled\")\n",
    "    \"\"\"\n",
    "    np.random.seed() # seed = None actually means we use the most randomness across processors\n",
    "    \n",
    "    # don't care about time_budget, just sample for long to get estimate\n",
    "    # of truth\n",
    "    if (est_type == \"truth\"):\n",
    "        result = usual_MCMC_est_crp(k, h, maxIter, data, sd, sd0, alpha,init_type)\n",
    "        states = None\n",
    "        num_sweeps = maxIter\n",
    "        num_est = 1\n",
    "        \n",
    "    elif (est_type == \"single\"):\n",
    "        if (init_type == \"all_same\"):\n",
    "            X = [np.zeros(data.shape[0], dtype=int)]\n",
    "        elif (init_type == \"crp_prior\"):\n",
    "            X = [crp_prior(data.shape[0],alpha)]\n",
    "        st = time.clock() # hopefully this is process-specific time\n",
    "        num_sweeps = 0\n",
    "        while (True):\n",
    "            X.append(gibbs_sweep_single(data, X[-1].copy(), sd, sd0, alpha))\n",
    "            num_sweeps += 1\n",
    "            time_elasped = time.clock() - st \n",
    "            if (time_elasped >= time_budget):\n",
    "                break\n",
    "        # with reasonable time_budget, this shouldn't happen, but just in case\n",
    "        if (len(X) <= k):\n",
    "            states = None\n",
    "            result = None\n",
    "            num_est = 0\n",
    "        else:\n",
    "            ests = [h(x) for x in X[k:]]\n",
    "            states = X \n",
    "            result = np.mean(ests, axis=0)\n",
    "            num_est = 1\n",
    "            \n",
    "    # as long as time budget hasn't ended, keep generating unbiased estimators\n",
    "    elif (est_type == \"coupled\"):\n",
    "        states = [] # each entry will be a coupling attempt, with the states of either chain and the meeting time\n",
    "        ests_ub = []\n",
    "        num_sweeps = None\n",
    "        num_est = 0\n",
    "        st = time.clock() # hopefully this is process-specific time\n",
    "        while (True):\n",
    "            est, tau, X, Y = unbiased_est_crp(k, h, m, data, sd, sd0, alpha, pi0_its, init_type, coupling)\n",
    "            if (time.clock() - st >= time_budget):\n",
    "                break\n",
    "            ests_ub += [est]\n",
    "            states.append((X,Y,tau))\n",
    "            num_est += 1\n",
    "        # this is unlikely to happen if we set time_budget to be reasonable, but\n",
    "        # just in case\n",
    "        if (num_est == 0):\n",
    "            result = None\n",
    "        else:\n",
    "            result = np.mean(ests_ub, axis=0)\n",
    "    \n",
    "    return states, result, num_sweeps, num_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_type = \"topK\"\n",
    "M = 10 # number of replicates \n",
    "time_budget = 500 # maximum time for each replicate\n",
    "maxIter = 100\n",
    "k = 10 # burn-in\n",
    "m = 20 # minimum iterations\n",
    "pi0_its = 5\n",
    "pool_size = 4\n",
    "init_type = \"crp_prior\"\n",
    "coupling='Optimal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top k proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savedir = \"estimation_results/gene_Ndata=%d_D=%d/\" %(Ndata,D)\n",
    "if not os.path.exists(savedir):\n",
    "    print(\"Will make directory %s\" %savedir)\n",
    "    os.makedirs(savedir)\n",
    "savedir = savedir + \"topK_\"\n",
    "topK = 10\n",
    "h = lambda z: prop_in_k_clusters(z, topK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth\n",
    "- variation across M replicates is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est_type = \"truth\"\n",
    "def simulate(_):\n",
    "    result = run_rep(est_type, h, maxIter, time_budget)\n",
    "    print(\"complete\")\n",
    "    return result\n",
    "with Pool(pool_size) as p:\n",
    "    results = p.map(simulate, range(M))\n",
    "    \n",
    "for res in results:\n",
    "    print(res)\n",
    "    \n",
    "savepath = savedir + \"truth_maxIter=%d.pkl\" %(maxIter)\n",
    "with open(savepath, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replicates of one chain time-budgetted run\n",
    "- Ndata=500, D=150 each sweep takes ~ 500s processor time (after potential multi-threading). In wall-clock time it felt like only ~2 minutes.\n",
    "- Ndata=200, D=50, in 300s processor time we can do ~ 20 sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est_type = \"single\"\n",
    "def simulate(_):\n",
    "    result = run_rep(est_type, h, maxIter, time_budget)\n",
    "    print(\"complete\")\n",
    "    return result\n",
    "with Pool(pool_size) as p:\n",
    "    results = p.map(simulate, range(M))\n",
    "    \n",
    "for res in results:\n",
    "    print(res)\n",
    "    \n",
    "savepath = savedir + \"single_initType=%s_maxTime=%d_burnin=%d.pkl\" %(init_type, time_budget,k)\n",
    "with open(savepath, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replicates of coupled chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "est_type = \"coupled\"\n",
    "def simulate(_):\n",
    "    result = run_rep(est_type, h, maxIter, time_budget)\n",
    "    print(\"complete\")\n",
    "    return result\n",
    "with Pool(pool_size) as p:\n",
    "    coupled_results = p.map(simulate, range(M))\n",
    "    \n",
    "for res in coupled_results:\n",
    "    print(res)\n",
    "    \n",
    "savepath = savedir + \"%s_coupled_initType=%s_maxTime=%d_burnin=%d_minIter=%d.pkl\" %(h_type, init_type, time_budget,k,m)\n",
    "with open(savepath, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = lambda z: co_cluster(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pool_size = 3\n",
    "def simulate(_):\n",
    "    result = run_rep(est_type=\"truth\", h, maxIter, time_budget)\n",
    "    print(\"complete\")\n",
    "    return result\n",
    "with Pool(pool_size) as p:\n",
    "    results = p.map(simulate, range(M))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicates of one chain time-budgetted run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicates of coupled chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
